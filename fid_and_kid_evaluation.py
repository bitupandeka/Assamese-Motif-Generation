# -*- coding: utf-8 -*-
"""FID and KID evaluation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zeKTiOdLVlKHN198jYlfE8sjqf7REFB9

# FID **Calculation**
"""

import numpy as np
import os
import tensorflow as tf
from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input
from scipy.linalg import sqrtm

dataset_path = "/content/drive/MyDrive/dataset_folder"

# Count the number of image files in the directory
image_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif')
image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(image_extensions)]
num_images = len(image_files)
print(f"Number of images in the dataset: {num_images}")

image_size = 64
batch_size = 16


def load_image(image_path, image_size):
    """
    Load and preprocess a single image.
    """
    image = tf.io.read_file(image_path)
    image = tf.image.decode_jpeg(image, channels=3)  # Change to decode_png if necessary
    image = tf.image.resize(image, [image_size, image_size], antialias=True)
    image = tf.clip_by_value(image / 255.0, 0.0, 1.0)  # Normalize to [0, 1]
    return image

# Function to load real images from the directory
def load_real_images(dataset_path, image_size, batch_size, num_images):
    """
    Load and preprocess real images from the specified directory.
    """
    image_paths = [os.path.join(dataset_path, fname) for fname in os.listdir(dataset_path)
                   if fname.lower().endswith(image_extensions)]

    if len(image_paths) < num_images:
        print(f"Warning: Only {len(image_paths)} images found, but {num_images} requested.")
        num_images = min(len(image_paths), num_images)

    # Shuffle and select only the needed number of paths
    np.random.shuffle(image_paths)
    image_paths = image_paths[:num_images]

    # Create a TensorFlow dataset from the image paths
    dataset = tf.data.Dataset.from_tensor_slices(image_paths)
    dataset = dataset.map(lambda x: load_image(x, image_size), num_parallel_calls=tf.data.AUTOTUNE)
    dataset = dataset.batch(batch_size)

    # Collect real images into a single NumPy array
    real_images = []
    for batch in dataset:
        real_images.append(batch.numpy())

    real_images = np.concatenate(real_images, axis=0)
    return real_images[:num_images]  # Ensure exactly num_images

# Function to calculate FID
def calculate_fid(real_images, generated_images, eps=1e-6, batch_size=16):
    # Load the InceptionV3 model
    inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))

    # Convert to NumPy arrays if they're TensorFlow tensors
    if isinstance(real_images, tf.Tensor):
        real_images = real_images.numpy()
    if isinstance(generated_images, tf.Tensor):
        generated_images = generated_images.numpy()

    # Ensure we have the same number of real and generated images
    n_samples = min(real_images.shape[0], generated_images.shape[0])
    real_images = real_images[:n_samples]
    generated_images = generated_images[:n_samples]

    print(f"Computing FID using {n_samples} images...")

    # Function to extract features in batches
    def extract_features(images):
        # Create dataset for batch processing
        dataset = tf.data.Dataset.from_tensor_slices(images)
        dataset = dataset.batch(batch_size)

        all_features = []
        total_batches = int(np.ceil(images.shape[0] / batch_size))

        for i, batch in enumerate(dataset):
            # Convert from [0,1] to [0,255] range
            batch = batch * 255.0

            # Resize to 299x299 for InceptionV3
            batch_resized = tf.image.resize(batch, (299, 299))

            # Preprocess for InceptionV3
            batch_preprocessed = preprocess_input(batch_resized)

            # Extract features
            features = inception_model.predict(batch_preprocessed, verbose=0)
            all_features.append(features)

            print(f"Processed batch {i+1}/{total_batches}")

        return np.vstack(all_features)

    # Extract features for real and generated images
    print("Extracting features from real images...")
    real_features = extract_features(real_images)

    print("Extracting features from generated images...")
    generated_features = extract_features(generated_images)

    # Calculate mean and covariance statistics
    mu_real = np.mean(real_features, axis=0)
    sigma_real = np.cov(real_features, rowvar=False)

    mu_gen = np.mean(generated_features, axis=0)
    sigma_gen = np.cov(generated_features, rowvar=False)

    # Add small epsilon to diagonal for numerical stability
    sigma_real += np.eye(sigma_real.shape[0]) * eps
    sigma_gen += np.eye(sigma_gen.shape[0]) * eps

    # Calculate mean difference term
    diff = mu_real - mu_gen
    mean_diff = np.sum(diff**2)

    # Calculate square root of product of covariances
    # Use scipy.linalg.sqrtm which is more numerically stable
    covmean = sqrtm(sigma_real.dot(sigma_gen))

    # Ensure covmean is real
    if np.iscomplexobj(covmean):
        covmean = covmean.real

    # Calculate FID
    fid = mean_diff + np.trace(sigma_real + sigma_gen - 2 * covmean)

    return fid

# Load real images from the directory
real_images = load_real_images(dataset_path, image_size, batch_size, num_images)

# Generate images with a specific number of diffusion steps
generated_images = model.generate(num_images=num_images, diffusion_steps=10)

# Ensure generated_images are in the same range (0-1)
generated_images = generated_images.numpy()

# Calculate FID
fid_score = calculate_fid(real_images, generated_images)
print(f"FID Score: {fid_score}")

"""# KID Calculation"""

from tensorflow.keras.applications.inception_v3 import InceptionV3
from tensorflow.keras import Model
from sklearn.metrics.pairwise import polynomial_kernel

def calculate_kid(real_images, generated_images, subset_size=50, n_subsets=10):
    # Input validation
    if real_images.shape[1:] != generated_images.shape[1:] or real_images.ndim != 4:
        raise ValueError("Images must have matching shape and 4 dimensions (N, H, W, C)")
    n_real, n_gen = len(real_images), len(generated_images)
    if n_real < subset_size or n_gen < subset_size:
        subset_size = min(n_real, n_gen) // 2 if min(n_real, n_gen) > 1 else 1
        print(f"Warning: Reduced subset_size to {subset_size} to match number of images.")

    # Load InceptionV3 feature extractor
    inception = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))
    inception.trainable = False

    # Resize and preprocess images
    real_images = tf.image.resize(real_images, [299, 299])
    generated_images = tf.image.resize(generated_images, [299, 299])
    # Handle input range [0, 1] or [0, 255] using tf.reduce_max
    real_max = tf.reduce_max(real_images)
    gen_max = tf.reduce_max(generated_images)
    # Scale to [0, 255] if max <= 1.0
    real_images = preprocess_input(real_images * 255.0 if real_max <= 1.0 else real_images)
    generated_images = preprocess_input(generated_images * 255.0 if gen_max <= 1.0 else generated_images)

    # Extract features
    real_features = inception.predict(real_images, batch_size=16, verbose=0)
    gen_features = inception.predict(generated_images, batch_size=16, verbose=0)

    # Compute KID using polynomial kernel (degree=3)
    kid_scores = []
    gamma = 1.0 / real_features.shape[1]  # 1/2048 for InceptionV3

    for _ in range(n_subsets):
        # Sample subsets without replacement
        real_sub = real_features[np.random.choice(n_real, subset_size, replace=False)]
        gen_sub = gen_features[np.random.choice(n_gen, subset_size, replace=False)]

        # Compute polynomial kernels
        kernel_real = polynomial_kernel(real_sub, gamma=gamma, degree=3, coef0=1)
        kernel_gen = polynomial_kernel(gen_sub, gamma=gamma, degree=3, coef0=1)
        kernel_cross = polynomial_kernel(real_sub, gen_sub, gamma=gamma, degree=3, coef0=1)

        # KID = mean(kernel_real) + mean(kernel_gen) - 2 * mean(kernel_cross)
        kid = kernel_real.mean() + kernel_gen.mean() - 2 * kernel_cross.mean()
        kid_scores.append(kid)

    # Return standard KID (sqrt of mean MMD^2)
    kid_mean = np.mean(kid_scores)
    return np.sqrt(kid_mean)

kid_score = calculate_kid(real_images, generated_images, subset_size=50, n_subsets=10)
print(f"KID: {kid_score:.4f}")